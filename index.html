<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="Learning to Drive Anyware via Model-Based Reannotation">
    <meta name="keywords" content="navigation, robotics, foundation model, dataset, MBRA, LogoNav">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning to Drive Anyware via Model-Based Reannotation</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-ZYH3N96LN5');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/slick.css">
    <link rel="stylesheet" href="./static/css/slick-theme.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/slick.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://svl.stanford.edu/projects/dvmpc/">
                            DVMPC: Deep Visual MPC-Policy Learning for Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/exaug-nav">
                            ExAug: Robot-conditioned Navigation Policies via Geometric Experience Augmentation
                        </a>                      
                        <a class="navbar-item" href="https://general-navigation-models.github.io/vint/index.html">
                            ViNT: A Foundation Model for Visual Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/sacson-review/home">
                            SACSoN: Scalable Autonomous Control for Social Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/selfi-rl/">
                            SELFI: Autonomous Self-improvement with Reinforcement Learning for Social Navigation
                        </a>
                        <a class="navbar-item" href="https://learning-language-navigation.github.io/">
                            LeLaN: Learning A Language-conditioned Navigation Policy from In-the-Wild Video
                        </a>                          
                        
                    </div>
                </div>
            </div>

        </div>
    </nav>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">Learning to Drive Anyware via Model-Based Reannotation</h1>                        
                                       
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a><sup>1, 2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://engineering.berkeley.edu/popup/lydia-ignatova/">Lydia Ignatova</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="http://ajaysridhar.com/">Kyle Stachowicz</a><sup>1</sup>,
                            </span><br>
                            <span class="author-block">
                                <a href="">Catherine Glossop</a><sup>1</sup>,
                            </span>                            
                            <span class="author-block">
                                <a href="https://cs.berkeley.edu/~svlevine">Sergey Levine</a><sup>1</sup>
                            </span>                            
                            <span class="author-block">
                                <a href="https://cs.berkeley.edu/~shah">Dhruv Shah</a><sup>1, 3</sup>,
                            </span>
                        </div>

                        <div class="is-size-6 publication-authors">
                            <span class="author-block"> <sup>1</sup> University of California Berkeley,   <sup>2</sup> Toyota Motor North America,   <sup>3</sup> Princeton University</span>
                        </div>
                        <br>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2410.03603.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                    <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                    </a>
                                </span>                            
                                <!-- Talk Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/-zTyhhu0NTY"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>YouTube</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/NHirose/Learning-to-Drive-Anywhere-via-MBRA"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github-alt"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://drive.google.com/file/d/1IazHcIyPGO7ENswz8_sGCIGBXF8_sZJK/view?usp=sharing"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i> </span>
                                        <span>Data</span>
                                    </a>
                                </span>

                                <!-- BibTex -->
                                <span class="link-block">
                                    <a href="./static/lelan.bib"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-quote-left"></i> </span>
                                        <span>BibTex</span>
                                    </a>
                                </span>

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop is-centered has-text-justified is-size-5">
            <div class="hero-body">
                <video id="teaser" disableRemotePlayback autoplay muted loop playsinline fetchpriority="high"
                    poster="./static/images/web_pull.jpeg">
                    <source src="./static/videos_mbra/web_pull.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
            We present a system for training long-horizon end-to-end navigation policies capable of generalizing to deployment in highly diverse outdoor and indoor environments. The result is a single end-to-end policy capable of navigation on the scale of hundreds of meters, while generalizing to a broad distribution of unstructured environments.
To accomplish this, we must make use of every source of data available. While previous efforts to train general navigation policies typically rely on centralized researcher-collected datasets, such data is fundamentally high-quality but limited in scale. We thus turn to crowd-sourced data, which is readily available in large quantities but is relatively low-quality. To address this, we train a simple, short-horizon ``reannotation'' policy, optionally taking advantage of actions from the high-quality dataset, and use this to relabel the low-quality dataset with near-expert actions connecting short-horizon sequences of states.
The result is LogoNav, a state-of-the-art general navigation policy capable of navigating over long horizons in complex outdoor environments. We test LogoNav's generalization capabilities by evaluating performance in six countries around the world and find it to be a highly capable navigator.

            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos_mbra/overview.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified has-text-centered">
            <h3 class="title is-4">Motivation</h3>
            <p>
            Our goal in this work is to enable end-to-end training of navigation policies for ground robots that can generalize broadly to a wide range of environments and follow reasonable conventions such as staying on paths and avoiding collisions. This requires large amounts of training data, which we can obtain from low-cost robotic platforms and crowd-sourced human data collection. While these sources can provide large amounts of data, this data is of low quality: the actions might not be consistently good, and even when they are, the sensors and state estimators on the robot might not allow for accurately estimating the next waypoint that corresponds to the human driver's intent. To address this, we propose Model-Based ReAnnotation (MBRA) to relabel data. 
            </p>          
            <h3 class="title is-4">Model-based Re-Annotation</h3>
            <p>
            In MBRA, at first, we apply the extended Kalman filter to denoise the action commands in the noisy dataset.
We also prepare the small publicly available navigation datasets with accurate action labels as a starting point for predicting actions. Filtering the dataset is not enough to mitigate noise, so we train a re-annotation model with model-based learning. Since model-based learning is robust for noisy data, we can incorporate both the accurate GNM dataset and the large but noisy FrodoBot 2k dataset to learn a re-annotation model. Since this model learns to predict actions between close image frames, we can directly use this model as a short-horizon goal-image conditioned navigation policy. Then, we use the re-annotation policy to create more useful action labels for trajectories in the large dataset. We use this data to train the long-horizon navigation policy, which can navigate the robot toward the goal pose about 50 meter away.
            </p>
            <img src="./static/images_mbra/system_overview.png" />       
            <h3 class="title is-4">Dataset</h3>
            <p>
            We use a <a href="https://huggingface.co/datasets/frodobots/FrodoBots-2K">FrodoBots-2k dataset</a>, which was collected in more than 10 cities around the world by the remote teleoperation. The FrodoBots-2k dataset includes the sequence of the front- and back-side camera image, GPS position, IMU sensor and wheel odometery for about 2000 hours. 
            </p>                
                 
            <h3 class="title is-4">Evaluation around the world</h3>
            <p>
            We use a wide variety of egocentric datasets to train on, including: 1) Indoor Navigation Dataset: image observations from mobile robot trajectories in office building environments (see <a href="https://cvgl.stanford.edu/gonet/dataset/">GO Stanford2</a>, <a href="https://svl.stanford.edu/projects/dvmpc/dataset/">GO Stanford4</a> and <a href="https://sites.google.com/view/sacson-review/huron-dataset?authuser=0">HuRoN(SACSoN)</a> Dataset), 2) \bf YouTube Tour Dataset: YouTube video data of tours in indoor and outdoor environments, and 3) Human-walking Dataset: data collected from walking with a camera in an indoor home setting and outdoor city environments.
            </p>    
            <center>    
            <video autoplay muted loop playsinline width="90%">
              <source src="static/videos_mbra/ev_map.mp4" type="video/mp4">
            </video>     
          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Experiment</h2>

                <!-- Interpolating. -->
                <h3 class="title is-4">Long-distance Navigation (LogoNav)</h3>
                <div class="content has-text-justified">
                  <p>
                  We evaluate our trained policy on target object navigation, which tasks the robot with navigating toward a visible target object from the current robot pose, in the various natural environments, which is not seen in the training datasets.
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="bww1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/long_dist_park.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/long_dist_campus.mp4" type="video/mp4">
                        </video>
                    </div>               
                </div>
            </div>            
        </div>
    </section>        

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Cross Embodiment Navigation</h3>
                <div class="content has-text-justified">
                  <p>
                  Given that our proposed method leverages in-the-wild videos recorded by a variety of cameras at different poses for training, it is inherently capable of generalizing to different embodiments. 
                  To rigorously evaluate our policies' cross-embodiment capabilities, we test our policy on the three different robot setups, 1) quadruped robot, GO1 with PCB-mounted fisheye camera, 2) same mobile robot with different cameras such as Intel Realsense D435i, PCB-mounted fisheye camera, and Ricoh Theta S.
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="go1_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/quad_6.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="d435_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/frodobot_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="height_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/vizbot_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/quad_2.mp4" type="video/mp4">
                        </video>
                    </div>                    
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/frodobot_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/quad_4.mp4" type="video/mp4">
                        </video>
                    </div>                         
                    
                    <div class="item item-rfs">
                        <video poster="" id="fisheye_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/vizbot_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/quad_1.mp4" type="video/mp4">
                        </video>
                    </div>                         
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/quad_3.mp4" type="video/mp4">
                        </video>
                    </div>                       
                    <div class="item item-go1-inside-1">
                        <video poster="" id="go1_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/frodobot_3.mp4" type="video/mp4">
                        </video>
                    </div>        
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/quad_5.mp4" type="video/mp4">
                        </video>
                    </div>                                             
                </div>
            </div>
        </div>
    </section>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Goal image-conditioned navigation (MBRA)</h3>
                <div class="content has-text-justified">
                  <p>
                  Even when there are objects in the same type as the target object, our method distinguishes them from the prompts and allows navigation toward the correct target objects, 
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="bww1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/goal_image_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos_mbra/goal_image_2.mp4" type="video/mp4">
                        </video>
                    </div>                  
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">  
      <div class="container is-max-desktop">    
        <h2 class="title">BibTeX</h2>
        <pre><code> @inproceedings{hirose25mbra,
        title={Learning to Drive Anywhere via Model-Based Reannotation},
        author={Noriaki Hirose and Lydia Ignatova and Kyle Stachowicz and Catherine Glossop and Sergey Levine and Dhruv Shah},
        booktitle={arXiv},
        year={2025}
        } </code></pre>            
      </div>
    </section>

    <br>
    <center class="is-size-10">
      The website (<a href="https://github.com/learning-language-navigation/learning-language-navigation.github.io.git">source code</a>) design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
                class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
